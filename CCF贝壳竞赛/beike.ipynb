{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import torch\n",
    "import transformers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import *\n",
    "import torch.utils.data as Data\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "output_dir = './models/'\n",
    "output_model_file = os.path.join(output_dir, WEIGHTS_NAME)\n",
    "output_config_file = os.path.join(output_dir, CONFIG_NAME)\n",
    "\n",
    "best_score = 0\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(['0\\t采荷一小是分校吧\\n',\n",
       "  '1\\t毛坯吗？\\n',\n",
       "  '2\\t你们的佣金费大约是多少和契税是多少。\\n',\n",
       "  '3\\t靠近川沙路嘛？\\n',\n",
       "  '4\\t这套房源价格还有优惠空间吗？\\n'],\n",
       " ['0\\t0\\t杭州市采荷第一小学钱江苑校区，杭州市钱江新城实验学校。\\t1\\n',\n",
       "  '0\\t1\\t是的\\t0\\n',\n",
       "  '0\\t2\\t这是5楼\\t0\\n',\n",
       "  '1\\t0\\t因为公积金贷款贷的少\\t0\\n',\n",
       "  '1\\t1\\t是呢\\t0\\n'])"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "train_question = open('./data/train.query.tsv','r',encoding = 'utf-8').readlines()\n",
    "train_answer = open('./data/train.reply.tsv','r',encoding='utf-8').readlines()\n",
    "train_question[:5],train_answer[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['0', '0', '杭州市采荷第一小学钱江苑校区，杭州市钱江新城实验学校。', '1']"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "train_answer[0].split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(['0\\t东区西区？什么时候下证？\\n',\n",
       "  '1\\t小学哪个\\n',\n",
       "  '2\\t看哪个？\\n',\n",
       "  '3\\t面积多少，什么户型\\n',\n",
       "  '4\\t什么时候能够看房呢？\\n'],\n",
       " ['0\\t0\\t我在给你发套\\n',\n",
       "  '0\\t1\\t您看下我发的这几套\\n',\n",
       "  '0\\t2\\t这两套也是金源花园的\\n',\n",
       "  '0\\t3\\t价钱低\\n',\n",
       "  '0\\t4\\t便宜的房子，一般都是顶楼\\n'])"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "test_question = open('./data/test.query.tsv','r',encoding = 'utf-8').readlines()\n",
    "test_answer = open('./data/test.reply.tsv','r',encoding='utf-8').readlines()\n",
    "test_question[:5],test_answer[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['0', '0', '我在给你发套']"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "test_answer[0].split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def data_preprocess(question_data,answer_data,train = True):\n",
    "    text = []\n",
    "    label = []\n",
    "    for question in question_data:\n",
    "        question_ids = question.split()[0]\n",
    "        question_text = question.split()[1]\n",
    "        for reply in answer_data:\n",
    "            content = [question_text]\n",
    "            if question_ids == reply.split()[0]:\n",
    "                reply_text = reply.split()[2]\n",
    "                content.append(reply_text)\n",
    "                text.append(content)\n",
    "                if train:\n",
    "                    label.append(int(reply.split()[-1]))\n",
    "    return text,label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(21585, 21585)"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "train_text ,train_label = data_preprocess(train_question,train_answer,train = True)\n",
    "len(train_text),len(train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(53757, 0)"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "test_text ,test_label = data_preprocess(test_question,test_answer,train = False)\n",
    "len(test_text),len(t_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('D:\\\\pretrain_models\\\\bert-base-chinese')\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased-finetuned-mrpc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_dict = tokenizer(\n",
    "            sentence1,\n",
    "            sentence2,\n",
    "            add_special_tokens = True,      # 添加 '[CLS]' 和 '[SEP]'\n",
    "            max_length = 64,             # 不够填充\n",
    "            padding = 'max_length',\n",
    "            truncation = True,             # 太长截断\n",
    "            return_tensors = 'pt', )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "metadata": {},
     "execution_count": 51
    }
   ],
   "source": [
    "encoder_dict['attention_mask'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(names, train_text, target): # name_list, sentence_list, label_list\n",
    "    input_ids, token_type_ids, attention_mask = [], [], []\n",
    "    for content in train_text:\n",
    "        encoded_dict = tokenizer(\n",
    "            content[0],        # 输入文本\n",
    "            content[1],\n",
    "            add_special_tokens = True,      # 添加 '[CLS]' 和 '[SEP]'\n",
    "            max_length = 64,             # 不够填充\n",
    "            padding = 'max_length',\n",
    "            truncation = True,             # 太长截断\n",
    "            return_tensors = 'pt',         # 返回 pytorch tensors 格式的数据\n",
    "        )\n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "        token_type_ids.append(encoded_dict['token_type_ids'])\n",
    "        attention_mask.append(encoded_dict['attention_mask'])\n",
    "\n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    token_type_ids = torch.cat(token_type_ids, dim=0)\n",
    "    attention_mask = torch.cat(attention_mask, dim=0)\n",
    "\n",
    "    input_ids = torch.LongTensor(input_ids)\n",
    "    token_type_ids = torch.LongTensor(token_type_ids)\n",
    "    attention_mask = torch.LongTensor(attention_mask)\n",
    "    target = torch.LongTensor(target)\n",
    "\n",
    "    return input_ids, token_type_ids, attention_mask, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids, token_type_ids, attention_mask, target = convert([], test_text, test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "attention_mask[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten() # [3, 5, 8, 1, 2, ....]\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "def save(model):\n",
    "    # save\n",
    "    torch.save(model.state_dict(), output_model_file)\n",
    "    model.config.to_json_file(output_config_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model, validation_dataloader):\n",
    "    model.eval()\n",
    "    eval_loss, eval_accuracy, nb_eval_steps = 0, 0, 0\n",
    "    for batch in validation_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        with torch.no_grad():\n",
    "            logits = model(batch[0], token_type_ids=batch[1], attention_mask=batch[2])[0]\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = batch[3].cpu().numpy()\n",
    "            tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "            eval_accuracy += tmp_eval_accuracy\n",
    "            nb_eval_steps += 1\n",
    "    print(\"Validation Accuracy: {}\".format(eval_accuracy / nb_eval_steps))\n",
    "    global best_score\n",
    "    if best_score < eval_accuracy / nb_eval_steps:\n",
    "        best_score = eval_accuracy / nb_eval_steps\n",
    "        save(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_eval():\n",
    "    input_ids, token_type_ids, attention_mask, labels = convert([], train_text, train_label)\n",
    "\n",
    "    train_inputs, val_inputs, train_labels, val_labels = train_test_split(input_ids, labels, random_state=1, test_size=0.1)\n",
    "    train_token, val_token, _, _ = train_test_split(token_type_ids, labels, random_state=1, test_size=0.1)\n",
    "    train_mask, val_mask, _, _ = train_test_split(attention_mask, labels, random_state=1, test_size=0.1)\n",
    "    \n",
    "    train_data = Data.TensorDataset(train_inputs, train_token, train_mask, train_labels)\n",
    "    train_dataloader = Data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    validation_data = Data.TensorDataset(val_inputs, val_token, val_mask, val_labels)\n",
    "    validation_dataloader = Data.DataLoader(validation_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained('D:\\\\NLP相关项目\\\\关系抽取项目实战\\\\bert-base-chinese', num_labels=2).to(device)\n",
    "\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'gamma', 'beta']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "        'weight_decay_rate': 0.01},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "        'weight_decay_rate': 0.0}]\n",
    "\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=1e-5)\n",
    "\n",
    "    for _ in range(2):\n",
    "        for i, batch in enumerate(train_dataloader):\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            loss = model(batch[0], token_type_ids=batch[1], attention_mask=batch[2], labels=batch[3])[0]\n",
    "            print(loss.item())\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if i % 10 == 0:\n",
    "              eval(model, validation_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred():\n",
    "    # load model\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(output_dir).to(device)\n",
    "    input_ids, token_type_ids, attention_mask, target = convert([], test_text, test_label)# whatever name_list and label_list\n",
    "    dataset = Data.TensorDataset(input_ids, token_type_ids, attention_mask)\n",
    "    loader = Data.DataLoader(dataset, 128, False)\n",
    "\n",
    "    pred_label = []\n",
    "    model.eval()\n",
    "    for i, batch in enumerate(loader):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        with torch.no_grad():\n",
    "            logits = model(batch[0], token_type_ids=batch[1], attention_mask=batch[2])[0]\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            preds = np.argmax(logits, axis=1).flatten()\n",
    "            pred_label.extend(preds)\n",
    "    \n",
    "    for i in range(len(pred_label)):\n",
    "        pred_label[i] = classes_list[pred_label[i]]\n",
    "\n",
    "    pd.DataFrame(data=pred_label, index=range(len(pred_label))).to_csv('./data/pred.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_list = [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}